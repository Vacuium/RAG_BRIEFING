# Retrieve Augmented Generation

How to build and maintain an architecture of Datastore and LMs (or LLMs)?

| Name                    | What to retrieve            | How to use retrieval | When to retrieve         | Pros                                    | Cons                                 | Ken's comment |
|-------------------------|-----------------------------|----------------------|--------------------------|-----------------------------------------|--------------------------------------|---------|
| [REALM](https://arxiv.org/pdf/2002.08909.pdf)                   | Text chunks                 | Input layer          | Once                     | Easy to implement                       | -                                    |Simplest one for inference. Training asynchronously.          |
| Retrieval-in-context LM \[[Ram et al., 2023](https://arxiv.org/pdf/2302.00083.pdf), [Shi et al., 2023](https://arxiv.org/pdf/2301.12652.pdf)\] | Text chunks                 | Input layer          | Every n tokens(n>1)      | More concise                            | Inefficient                          |Just shorten the query window of input text to make query more concise         |
| [RETRO](https://arxiv.org/pdf/2112.04426.pdf)                   | Text chunks                 | Intermediate layers  | Every n tokens(n>1)      | Flexible, efficient                     | Complex, fine-tuning neeeded         |Chunk input and retrieve corresponding documents. Insert encoded documents in Chunked Cross Attention (CCA) layer between attention layer and Feed-forward layer in traditional transformer block. Make the LM model fit the retrieval reasult.          |
| [kNN-LM](https://arxiv.org/pdf/1911.00172.pdf)                  | Tokens                      | Output layer         | Every token              | More fine-grained; efficient            | Expensive datastore                  |Can be seen as a completion from kNN retrieval result when LM is not sure about the next output token.        |
| [FLARE](https://arxiv.org/pdf/2305.06983.pdf)                   | Text chunks                 | Input layer          | Every n tokens(adaptive) | More efficient                          | Decision may not be optimal          |Retrieve from datastore when LM output something masked. Let LM decide when to retrieve.          |
| Adaptive kNN-LM \[[He et al., 2021](https://arxiv.org/pdf/2109.04212.pdf), [Alon et al., 2022](https://arxiv.org/pdf/2201.12431.pdf)\]         | Tokens                      | Output layer         | Every n tokens(adaptive) | More efficient                          | Decision may not be optimal          |Retrieve and output next token everytime a token output when LM is not sure about the next output token.           |
| [Entities as experts](https://arxiv.org/pdf/2004.07202.pdf), [Mention Memory](https://arxiv.org/pdf/2110.06176.pdf)     | Entities or entity mentions | Intermediate layers  | Every entity mentions    | More efficient for entitiy-centric task | Entity(or glossary) detection needed |Quite like adding glossary          |
| Retrieval for long-range LM \[[Wu et al., 2022](https://arxiv.org/pdf/2203.08913.pdf), [Rubin & Berant. 2023](https://arxiv.org/pdf/2306.13421.pdf)\] | Text chunks from the input  | Intermediate layers  | Once or every n tokens   | -                                       | -                                    |Not understand yetðŸ¤”         |